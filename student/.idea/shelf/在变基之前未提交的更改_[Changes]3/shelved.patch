Index: crawler_car315case.py
===================================================================
diff --git a/crawler_car315case.py b/crawler_car315case.py
deleted file mode 100644
--- a/crawler_car315case.py	(revision 950a3822570582862afddde6fb584cae7dda45e4)
+++ /dev/null	(revision 950a3822570582862afddde6fb584cae7dda45e4)
@@ -1,48 +0,0 @@
-from selenium import webdriver
-from selenium.webdriver.common.by import By
-import time
-import random
-
-# option = webdriver.ChromeOptions()
-# option.add_argument("headless")
-# browser = webdriver.Chrome(chrome_options=option)
-browser = webdriver.Chrome()
-
-browser.get('http://tousu.315che.com/tousulist/serial/20/')
-
-
-# /html/body/div[1]/div[2]/div[2]/div[1]/div[2]/div[2]/ul/li[1]
-def get_all_subpages(home_link: str) -> list:
-    """get all subpages"""
-    urls_list = []
-    browser.get(home_link)
-    time.sleep(random.uniform(1, 3))
-    a = browser.find_elements(By.CLASS_NAME, 'tousu-filter-list')[0]
-    tousu_filter_list = a.find_elements(By.TAG_NAME, 'a')
-    for i in tousu_filter_list:
-        urls_list.append(i.get_attribute("href"))
-    # print(urls_list)
-    return urls_list
-
-
-def all_page_links(url: str) -> None:
-    """all page urls"""
-    browser.get(url)
-    time.sleep(random.uniform(1, 3))
-    page_num = browser.find_elements(By.CLASS_NAME, 'pag-tip')[0].text.split('共')[1].split('页')[0]
-    page_num = int(page_num)
-    for n in range(1, page_num + 1):
-        p = url_string_rules(url, n)
-        get_all_subpages(p)
-
-
-def url_string_rules(url: str, n: int) -> str:
-    """string rules"""
-    t = url.split('http://tousu.315che.com/tousulist/serial/')[1]
-    result_url = url.replace(url.split('http://tousu.315che.com/tousulist/serial/')[1], '%s0/0/0/%s.htm' % (t, n))
-    return result_url
-
-
-def get_rank(url: str) -> str:
-    
-# print(all_page_links('http://tousu.315che.com/tousulist/serial/20/'))
Index: t220419.py
===================================================================
diff --git a/t220419.py b/t220419.py
deleted file mode 100644
--- a/t220419.py	(revision 950a3822570582862afddde6fb584cae7dda45e4)
+++ /dev/null	(revision 950a3822570582862afddde6fb584cae7dda45e4)
@@ -1,98 +0,0 @@
-import json
-import ipaddress
-import httpx
-from lxml import etree
-
-s = '[{"port": 80, "score": 150.549, "update_time": 1652509227.0, "anonymous": 1, "download_speed_average": 56917.2, "response_time_average": 6.10687, "country_code": "US", "ip": "162.214.202.170", "working_average": 87.8431, "country_name": "United States"}]'
-
-
-def proxy_check(ip_address: str, port: int) -> bool:
-    """
-    check whether the proxy ip and port are valid
-    :param ip_address: proxy ip value
-    :param port: proxy port value
-    :return: True or False
-    """
-    try:
-        ipaddress.ip_address(ip_address)
-        _port = int(port)
-        if _port > 65535 or _port <= 0:
-            raise ValueError(f'Invalid port {port}')
-    except ValueError:
-        return False
-    return True
-
-
-def demo(s):
-    infos = json.loads(s)
-    items = []
-    for info in infos:
-        try:
-            ip = info.get('ip')
-            port = info.get('port')
-            if not proxy_check(ip, port):
-                continue
-            items.append(f'http://{ip}:{port}')
-            items.append(f'https://{ip}:{port}')
-        except Exception as ex:  # pylint: disable=broad-except
-            print('Parse info error %s.%s' % ex, info)
-    return items
-
-
-def parse(rows_rule, row_start, row_end):
-    url = 'http://www.66ip.cn/'
-    response = httpx.get(url)
-    html = etree.HTML(response.text)
-    items = []
-    rows = html.xpath(rows_rule)[row_start:]
-    if row_end is not None:
-        rows = rows[:row_end]
-
-    for row in rows:
-        row_html = etree.tostring(row).decode()
-        if '透明' in row_html or 'transparent' in row_html.lower():
-            continue
-        proxy_ip = parse_row(row=row)
-        if proxy_ip:
-            items.extend(proxy_ip)
-    return items
-
-
-def parse_row(row) -> list[str] | None:
-    """
-    parse a row
-    :param row:
-    :return: 127.0.0.1:1080 / ''
-    """
-    row_html = etree.tostring(row).decode()
-    try:
-        proxy_ip = ''
-        if columns_rule:
-            columns = row.xpath(columns_rule)
-            if columns:
-                _ip = columns[ip_position]
-                proxy_ip = _ip.text
-                if ip_rule:
-                    proxy_ip = _ip.xpath(ip_rule)[0]
-                if port_position:
-                    port = columns[port_position]
-                    port_str = port.text
-                    if port_rule:
-                        port_str = port.xpath(port_rule)[0]
-                    proxy_ip = f'{proxy_ip}:{port_str}'
-        else:
-            proxy_ip = row_html
-        if proxy_ip and proxy_check(*proxy_ip.split(':')):
-            return [
-                f'http://{proxy_ip}',
-                f'https://{proxy_ip}'
-            ]
-    # I'm not sure if it's going to cause anything else.
-    # But I want to avoid a problem that could cause a program to fail
-    except Exception as ex:  # pylint: disable=broad-except
-        print('Parse row error %s. \n%s', ex, row_html)
-    return None
-
-
-print(parse('//tr', 1, None))
-# print(demo(s))
