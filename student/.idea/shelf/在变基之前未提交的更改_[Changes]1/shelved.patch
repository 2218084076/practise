Index: t220419.py
===================================================================
diff --git a/t220419.py b/t220419.py
deleted file mode 100644
--- a/t220419.py	(revision bca27d0a1913315107987ebbb8964bd93f2a10bb)
+++ /dev/null	(revision bca27d0a1913315107987ebbb8964bd93f2a10bb)
@@ -1,98 +0,0 @@
-import json
-import ipaddress
-import httpx
-from lxml import etree
-
-s = '[{"port": 80, "score": 150.549, "update_time": 1652509227.0, "anonymous": 1, "download_speed_average": 56917.2, "response_time_average": 6.10687, "country_code": "US", "ip": "162.214.202.170", "working_average": 87.8431, "country_name": "United States"}]'
-
-
-def proxy_check(ip_address: str, port: int) -> bool:
-    """
-    check whether the proxy ip and port are valid
-    :param ip_address: proxy ip value
-    :param port: proxy port value
-    :return: True or False
-    """
-    try:
-        ipaddress.ip_address(ip_address)
-        _port = int(port)
-        if _port > 65535 or _port <= 0:
-            raise ValueError(f'Invalid port {port}')
-    except ValueError:
-        return False
-    return True
-
-
-def demo(s):
-    infos = json.loads(s)
-    items = []
-    for info in infos:
-        try:
-            ip = info.get('ip')
-            port = info.get('port')
-            if not proxy_check(ip, port):
-                continue
-            items.append(f'http://{ip}:{port}')
-            items.append(f'https://{ip}:{port}')
-        except Exception as ex:  # pylint: disable=broad-except
-            print('Parse info error %s.%s' % ex, info)
-    return items
-
-
-def parse(rows_rule, row_start, row_end):
-    url = 'http://www.66ip.cn/'
-    response = httpx.get(url)
-    html = etree.HTML(response.text)
-    items = []
-    rows = html.xpath(rows_rule)[row_start:]
-    if row_end is not None:
-        rows = rows[:row_end]
-
-    for row in rows:
-        row_html = etree.tostring(row).decode()
-        if '透明' in row_html or 'transparent' in row_html.lower():
-            continue
-        proxy_ip = parse_row(row=row)
-        if proxy_ip:
-            items.extend(proxy_ip)
-    return items
-
-
-def parse_row(row) -> list[str] | None:
-    """
-    parse a row
-    :param row:
-    :return: 127.0.0.1:1080 / ''
-    """
-    row_html = etree.tostring(row).decode()
-    try:
-        proxy_ip = ''
-        if columns_rule:
-            columns = row.xpath(columns_rule)
-            if columns:
-                _ip = columns[ip_position]
-                proxy_ip = _ip.text
-                if ip_rule:
-                    proxy_ip = _ip.xpath(ip_rule)[0]
-                if port_position:
-                    port = columns[port_position]
-                    port_str = port.text
-                    if port_rule:
-                        port_str = port.xpath(port_rule)[0]
-                    proxy_ip = f'{proxy_ip}:{port_str}'
-        else:
-            proxy_ip = row_html
-        if proxy_ip and proxy_check(*proxy_ip.split(':')):
-            return [
-                f'http://{proxy_ip}',
-                f'https://{proxy_ip}'
-            ]
-    # I'm not sure if it's going to cause anything else.
-    # But I want to avoid a problem that could cause a program to fail
-    except Exception as ex:  # pylint: disable=broad-except
-        print('Parse row error %s. \n%s', ex, row_html)
-    return None
-
-
-print(parse('//tr', 1, None))
-# print(demo(s))
Index: crawler_car315case.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/crawler_car315case.py b/crawl_car315case.py
rename from crawler_car315case.py
rename to crawl_car315case.py
--- a/crawler_car315case.py	(revision bca27d0a1913315107987ebbb8964bd93f2a10bb)
+++ b/crawl_car315case.py	(date 1652750900506)
@@ -1,19 +1,41 @@
+import random
+import time
+
 from selenium import webdriver
 from selenium.webdriver.common.by import By
-import time
-import random
 
 # option = webdriver.ChromeOptions()
 # option.add_argument("headless")
 # browser = webdriver.Chrome(chrome_options=option)
 browser = webdriver.Chrome()
 
-browser.get('http://tousu.315che.com/tousulist/serial/20/')
+
+def main():
+    """main"""
+    all_brands = get_all_brands('http://tousu.315che.com/tousulist/serial/20/')
+    for i in all_brands:
+        page_num = all_page_links(i)
+        for j in range(1, page_num):
+            p = url_string_rules(j, j)
+            get_all_subpages(p)
 
 
-# /html/body/div[1]/div[2]/div[2]/div[1]/div[2]/div[2]/ul/li[1]
+def get_all_brands(url: str) -> list:
+    l = []
+    browser.get(url)
+    time.sleep(random.randint(1, 3))
+    div = browser.find_element(By.ID, 'letterTabList')
+    a = div.find_elements(By.TAG_NAME, 'a')
+    for i in a:
+        brand_url = i.get_attribute('href')
+        l.append(brand_url)
+        print('get %s ' % brand_url)
+        all_page_links(brand_url)
+    return l
+
+
 def get_all_subpages(home_link: str) -> list:
-    """get all subpages"""
+    """get all subpages 获取子页面 link"""
     urls_list = []
     browser.get(home_link)
     time.sleep(random.uniform(1, 3))
@@ -25,15 +47,16 @@
     return urls_list
 
 
-def all_page_links(url: str) -> None:
-    """all page urls"""
+def all_page_links(url: str) -> int:
+    """
+    all page urls
+    return page number
+    """
     browser.get(url)
     time.sleep(random.uniform(1, 3))
     page_num = browser.find_elements(By.CLASS_NAME, 'pag-tip')[0].text.split('共')[1].split('页')[0]
     page_num = int(page_num)
-    for n in range(1, page_num + 1):
-        p = url_string_rules(url, n)
-        get_all_subpages(p)
+    return page_num
 
 
 def url_string_rules(url: str, n: int) -> str:
@@ -42,7 +65,4 @@
     result_url = url.replace(url.split('http://tousu.315che.com/tousulist/serial/')[1], '%s0/0/0/%s.htm' % (t, n))
     return result_url
 
-
-def get_rank(url: str) -> str:
-    
 # print(all_page_links('http://tousu.315che.com/tousulist/serial/20/'))
